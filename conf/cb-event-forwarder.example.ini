[bridge]

###########
#General Config Options
##########

#
# How many process pools should the script spin up to
# process events off of the bus.
message_processor_count=4

#
# This is a name that gets populated in all JSON objects
# as "cb_server".  This can help distinguish messages when
# data from multiple Cb Response deployments are merged
# into a single source
server_name=cbserver

# enable extra debugging output
debug=0

# port for HTTP diagnostics
http_server_port=33706

#
# Bus Connection Options
#
# There are two deployment options:
#
# 1) For small deployments, or for low volume subscriptions (such as new binaries, feed/watchlist hits),
#    you can install this connector directly on the Cb Response server. In this case, leave the following
#    three configuration options blank and the service will connect to the local RabbitMQ instance using
#    the credentials from the /etc/cb/cb.conf file.
#
# 2) For larger deployments, or for high volume subscriptions (such as raw endpoint events), it is recommended
#    to install this connector on its own dedicated machine. In this case, fill the following three configuration
#    options with the RabbitMQUser, RabbitMQPassword, and the IP/hostname of the Cb Response server or master
#    node respectively. You will have to ensure that your host can connect to TCP port 5004 on the Cb Response
#    server.
#
rabbit_mq_username=
rabbit_mq_password=
cb_server_hostname=

#
# The cb-event-forwarder can optionally place deep links into the JSON or LEEF output so users can have
# one-click access to process, binary, or sensor context. For example, a watchlist process hit will now include:
#  .docs[0].link_sensor: https://cbtests/#/host/7
#  .docs[0].link_process: https://cbtests/#analyze/00000007-0000-0fd4-01d1-209aa22a57ee/1
#  .docs[0].link_parent: https://cbtests/#analyze/00000007-0000-0fc8-01d1-209aa208f788/1
#  .docs[0].link_process_md5: https://cbtests/#/binary/445C3E95C8CB05403AEDAEC3BAAA3A1D
#
# Raw endpoint events will include a "link_process", and binary watchlist hits will include a "link_md5"
#
# To enable these links, uncomment and place the base URL of your Cb Response server into the cb_server_url variable
# below.

# The cb_server_url is also used for any post processing that requires the Cb Response Server's REST API access.
#
# cb_server_url=https://my.company.cb.server

#
# Post Processing Options
#
# Supported post processing:
#
# 1) report_title in feed hits
#
# Post processing requires cb_server_url, api_verify_ssl, and api_token to be set.  The post processed messages are
# dispatched to retrieve additional information from the Cb Response REST API.  Once the information is retrieved they
# are placed in the ouput queue.
#
# SSL/TLS verification for connections to the Cb Response REST API
#
# api_verify_ssl=false
#
# The API Token to use when querying the Cb Response REST API
#
# api_token=

#########
# Output Options
#########

#
# Configure the specific output.
# Valid options are: 'udp', 'tcp', 'file', 'stdout', 's3'
#
#  udp - Have the events sent over a UDP socket
#  tcp - Have the events sent over a TCP socket
#  file - Output the events to a rotating file
#  s3 - Place in S3 bucket (not officially supported)
#  syslog - Send the events to a syslog server
#
output_type=file

# Configure the output format
# valid options are: 'leef', 'json'
#
# default is 'json'
# Use 'leef' for pushing events to IBM QRadar, 'json' otherwise
#
output_format=json

#
# Output specific configuration
# These only have meaning if the option
# is enabled
#

# default to /var/cb/data/event_bridge_output.json
outfile=/var/cb/data/event_bridge_output.json

# tcpout=IP:port - ie 1.2.3.5:8080
tcpout=

# udpout=IP:port - ie 1.2.3.5:8080
udpout=

# options for S3 support
# s3out: can be an S3 bucket name (defaults to us-east-1 region)
#        or (temp-file-directory):(region):(bucket-name)
#        by default, the temp-file-directory is /var/cb/data/event-forwarder.
#
# for more s3 options, see the [s3] section below.
s3out=

# options for syslog output
# syslogout:
#   uses the format <protocol>:<hostname>:<port>
#   where <protocol> can be:
#      tcp+tls:      TCP over TLS/SSL
#      tcp:          plaintext TCP
#      udp:          plaintext UDP

# example:
#   tcp+tls:syslog.company.com:514
syslogout=

# options for HTTP output
# httpout:
#   uses the format <temporary file location>:<HTTP URL>
#   where the temporary file location is optional; defaults to /var/cb/data/event-forwarder
#
# for more http options, see the [http] section below.
#
# examples:
#   httpout=/tmp/http_out:https://http-endpoint.company.local/api/submit
#   httpout=https://http-endpoint.company.local/api/submit
httpout=

#########
# Configuration for which events are captured
#
# To specify multiple events use comma as a separator
# e.g. events_raw_sensor=ingress.event.process,ingress.event.procstart
#
#
# For more info on supported event types see
# https://github.com/carbonblack/cbapi/tree/master/server_apis
#
# Note: To enable raw sensor events you must also edit the Cb Response config
#  file (cb.conf) and enable the appropriate events by changing the config
#  option DatastoreBroadcastEventTypes.
#
# If you intend to enable raw sensor events on a Cb Enterprise Response 5.2+ server,
#  consider using the "raw sensor exchange" for enhanced performance and reduced
#  load on the Cb server. The raw exchange forwards the compressed data received
#  from the endpoint sensor unchanged to the event-forwarder. The raw sensor
#  exchange should be used if you are forwarding moduleload, filemod, or regmod
#  endpoint events as these represent the vast majority of the event types.
#
# To use the "raw sensor exchange", first uncomment the "use_raw_sensor_exchange"
#  option below:
#use_raw_sensor_exchange=true
#
#  then enable the "raw sensor exchange" in cb.conf:
#  EnableRawSensorDataBroadcast=true
#
# Note: To enable binaryinfo. events you need to enable EnableSolrBinaryInfoNotifications=True
#  within the cb.conf
#########

# Raw Sensor (endpoint) Events
# Includes:
#   ingress.event.process
#   ingress.event.procstart
#   ingress.event.netconn
#   ingress.event.procend
#   ingress.event.childproc
#   ingress.event.moduleload
#   ingress.event.module
#   ingress.event.filemod
#   ingress.event.regmod
#   ingress.event.tamper
#   ingress.event.crossprocopen
#   ingress.event.remotethread
#   ingress.event.processblock
#   ingress.event.emetmitigation
#   ALL for all of the above
#   0 - to disable all raw sensor events.
events_raw_sensor=ALL

# Watchlist Hits
# Includes:
#  watchlist.hit.process
#  watchlist.hit.binary
#  watchlist.storage.hit.process
#  watchlist.storage.hit.binary
# Note: As of version 5.2, the routing keys are different in RabbitMQ
# if you want to only subscribe to watchlist.storage.hit.process (for example),
# your configuration should be
# events_watchlist=watchlist.*.storage.hit.process note the '*' after the '.'
# Internally all watchlists show up with their database ID
# ex: watchlist.12.storage.hit.process, you'll miss them without the '*'
# (asterisk)
events_watchlist=ALL

# Feed Hits
# Includes:
#   feed.ingress.hit.process
#   feed.ingress.hit.binary
#   feed.ingress.hit.host
#   feed.storage.hit.process
#   feed.storage.hit.binary
#   feed.query.hit.process
#   feed.query.hit.binary
#   ALL for all of the above
#   0 - to disable all raw sensor events
# Note: As of version 5.2, the routing keys are different in RabbitMQ
# if you want to only subscribe to feed.storage.hit.process (for example), your
# configuration should be
# events_feed=feed.*.storage.hit.process note the '*' after the '.'
# Internally all feeds show up with their database ID
# ex: feed.12.storage.hit.process, you'll miss them without the '*' (asterisk)
events_feed=ALL

# Alert Events
# Includes:
#   alert.watchlist.hit.ingress.process
#   alert.watchlist.hit.ingress.binary
#   alert.watchlist.hit.ingress.host
#   alert.watchlist.hit.query.process
#   alert.watchlist.hit.query.binary
#   ALL for all of the above
#   0 - to disable all raw sensor events
events_alert=ALL

# Binary Observed Events
# Includes:
#   binaryinfo.observed
#   binaryinfo.host.observed
#   binaryinfo.group.observed
events_binary_observed=ALL

# Binary Upload Events
# Includes:
#   binarystore.file.added
events_binary_upload=ALL

#########
# S3 configuration section
#
# The following are advanced configuration options for uploading output to Amazon S3 buckets.
#########

[s3]
# By default the S3 output type will initiate a connection to the remote service every five minutes, or when
#  the temporary file containing the event output reaches 10MB.

# Set the default timeout period in seconds. By default, cb-event-forwarder will contact the remote service every five
#  minutes (300 seconds)
# bundle_send_timeout=300

# Send empty updates? By default, cb-event-forwarder will send an empty update every bundle_send_timeout seconds.
#  if this is set to false, then the cb-event-forwarder will not initiate a connection to the remote service unless
#  there are events to send.
# upload_empty_files=true

# Set the maximum file size before the events must be flushed to the remote service. The default is 10MB.
# bundle_size_max=10485760

# Uncomment server_side_encryption below to enable SSE on uploaded files to your S3 bucket
# server_side_encryption=AES256

# Set the following ACL policy on all files uploaded to your S3 bucket
# acl_policy=bucket-owner-full-control

# Use the following credential profile to connect to S3.
# See http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html for more information on
# AWS credential storage and credential profiles. By default, the S3 output will use the default
# credential storage location of $HOME/.aws/credentials with the "default" credential profile.
#
# The credential_profile can be specified as the profile name, using the default storage location, or
# you can also specify the filename where the credentials are stored by using a colon separated format:
# for example, to look for the AWS credentials in the file /etc/cb/aws.creds, and use profile "production":
# set credential_profile to /etc/cb/aws.creds:production.

# credential_profile=default

# Use the following to create event forwarder logs under a specified object prefix
# If specified logs will be stored under <bucketname>/<object_prefix>/event-forwarder.<timestamp>
# This is useful if multiple forwarders are to use the same s3 bucket
# object_prefix=objectname

[syslog]
# Uncomment ca_cert to specify a file containing PEM-encoded CA certificates for verifying the peer
# server when using TLS+TCP syslog
# ca_cert=/etc/cb/integrations/event-forwarder/ca-certs.pem

# Uncomment tls_verify and set to "false" in order to disable verification of the peer server certificate
# when using TLS+TCP syslog
# tls_verify=false

# Uncomment client_key and client_cert and set to files containing PEM-encoded private key and public
# certificate when using client TLS certificates when using TLS+TCP syslog
# client_key=/etc/cb/integrations/event-forwarder/client-key.pem
# client_cert=/etc/cb/integrations/event-forwarder/client-cert.pem

[http]
# By default the HTTP POST output type will initiate a connection to the remote service every five minutes, or when
#  the temporary file containing the event output reaches 10MB.

# Set the default timeout period in seconds. By default, cb-event-forwarder will contact the remote service every five
#  minutes (300 seconds)
# bundle_send_timeout=300

# Send empty updates? By default, cb-event-forwarder will send an empty update every bundle_send_timeout seconds.
#  if this is set to false, then the cb-event-forwarder will not initiate a connection to the remote service unless
#  there are events to send.
# upload_empty_files=true

# Set the maximum file size before the events must be flushed to the remote service. The default is 10MB.
# bundle_size_max=10485760

# Override the default template used for posting JSON to the remote service.
# The template language is Go's text/template; see https://golang.org/pkg/text/template/
# The following placeholders can be used:
#  {{.FileName}} - the filename of the current event-forwarder file being uploaded (for example event-forwarder.2016-08-11T01:01:01Z)
#  {{.Events}} - the list of events being uploaded as a "range". Each event has the following placeholder:
#   {{.EventText}} - the event itself, as a JSON dictionary.
#                    Note that a comma is added to each event (except the last) to make the list proper JSON.
# The default template for JSON is:
# http_post_template={"filename": "{{.FileName}}", "service": "carbonblack", "alerts":[{{range .Events}}{{.EventText}}{{end}}]}

# Override the content-type sent to the remote service through the HTTP Content-Type header.
#  The default content-type for JSON output is application/json.
# content_type=application/json

# Uncomment ca_cert to specify a file containing PEM-encoded CA certificates for verifying the peer server
# ca_cert=/etc/cb/integrations/event-forwarder/ca-certs.pem

# Uncomment tls_verify and set to "false" in order to disable verification of the peer server certificate
# tls_verify=false

# Uncomment client_key and client_cert and set to files containing PEM-encoded private key and public
#  certificate when using client TLS certificates
# client_key=/etc/cb/integrations/event-forwarder/client-key.pem
# client_cert=/etc/cb/integrations/event-forwarder/client-cert.pem

# Uncomment authorization_token to place a value in the outgoing HTTP "Authorization" header
#  (used in HTTP Basic Authentication). See https://en.wikipedia.org/wiki/Basic_access_authentication
#  for more information. By default no Authorization header is sent.
# authorization_token=Basic QWxhZGRpbjpPcGVuU2VzYW1l


[kafka]
# Uncomment when using kafka brokers. Broker configuration should be a comma-separated
# list of brokers with ports.
# e.g.
# brokers=kakfa01:9092,kafka02:9092
# It is not necessary to put all of your brokers here, just enough to dependably
# bootstrap the producer

# Optional additional suffix to add to the name of the topics.
# By default the topic published to will be the type of the event, e.g. "procstart'
# Must be uncommented when using kafka output but can be left blank
# topic_suffix=-test