[bridge]

###########
#General Config Options
##########

###
#
#enables data compression (gzip)
#
compress_data=false

#
# How many process pools should the script spin up to
# process events off of the bus.
message_processor_count=10

#
# This is a name that gets populated in all JSON objects
# as "cb_server".  This can help distinguish messages when
# data from multiple Cb Response deployments are merged
# into a single source
server_name=cbserver

#
# enables extra debugging output and stores zip bundles if errors occur during processing
#
#debug=0
#debug_store=/tmp

# port for HTTP diagnostics
http_server_port=33706

#
#Control Audit logging
#
#audit_log=false

#
# Bus Connection Options
#
# There are two deployment options:
#
# 1) For small deployments, or for low volume subscriptions (such as new binaries, feed/watchlist hits),
#    you can install this connector directly on the Cb Response server. In this case, leave the following
#    three configuration options blank and the service will connect to the local RabbitMQ instance using
#    the credentials from the /etc/cb/cb.conf file.
#
# 2) For larger deployments, or for high volume subscriptions (such as raw endpoint events), it is recommended
#    to install this connector on its own dedicated machine. In this case, fill the following three configuration
#    options with the RabbitMQUser, RabbitMQPassword, and the IP/hostname of the Cb Response server or master
#    node respectively. You will have to ensure that your host can connect to TCP port 5004 on the Cb Response
#    server.
#
rabbit_mq_username=cb
rabbit_mq_password=cb
cb_server_hostname=rabbitmq
rabbit_mq_port=5672

# Rabbit MQ Acking
# RabbitMQ has two options for acking events off of the message bus, automatic and manual.
# In automatic acknowledgement mode, a message is considered to be successfully delivered
# immediately after it is sent. This mode trades off higher throughput (as long as the consumers can keep up)
# for reduced safety of delivery and consumer processing. With manual acking, the
# consumer must ack each individual event sent by the producer and an event will not
# be considered sucessfully received until the consumer sends an ack to the producer.
# See https://www.rabbitmq.com/confirms.html for a more detailed explination.
# Automatic acking is recommended as not all environments can keep up with the rate of acking
# that is required. If manual acking is desired, please be sure to make sure that your
# consumers can keep up with the number of acks necessary. This can be done by increasing
# message_processor_count, using EnableRawSensorDataBroadcast vs events_raw_sensor,
# and using rabbit_mq_queue_name while running multiple instances of the Event Forwarder.
# If rabbit_mq_automatic_acking is set to true then automatic mode is used, if
# rabbit_mq_automatic_acking is false then manual mode will be used. The default is true.
#
rabbit_mq_automatic_acking=true


# Rabbit MQ queue Name
# The RabbitMQ queue name is the name of the queue that is created on the RabbitMQ server
# from which the event forwarder will receive events. By default this name is made up of a static
# string, the hostname of the machine it's running on, and the pid of the event forwarder process.
# This might not be desired in high throughput environments where it's necessary to run multiple instances
# of the event forwarder in order to keep up with the volume of events coming off of the RabbitMQ event bus.
# By setting rabbit_mq_queue_name to a static string the queue name will be the same across all running
# instances of the event forwarder, allowing multiple instances of the event forwarder to consume from the
# same RabbitMQ queue. The default is an empty string.
#
rabbit_mq_queue_name=

#
# The cb-event-forwarder can optionally place deep links into the JSON or LEEF output so users can have
# one-click access to process, binary, or sensor context. For example, a watchlist process hit will now include:
#  .docs[0].link_sensor: https://cbtests/#/host/7
#  .docs[0].link_process: https://cbtests/#analyze/00000007-0000-0fd4-01d1-209aa22a57ee/1
#  .docs[0].link_parent: https://cbtests/#analyze/00000007-0000-0fc8-01d1-209aa208f788/1
#  .docs[0].link_process_md5: https://cbtests/#/binary/445C3E95C8CB05403AEDAEC3BAAA3A1D
#
# Raw endpoint events will include a "link_process", and binary watchlist hits will include a "link_md5"
#
# To enable these links, uncomment and place the base URL of your Cb Response server into the cb_server_url variable
# below.

# The cb_server_url is also used for any post processing that requires the Cb Response Server's REST API access.
#
# cb_server_url=https://my.company.cb.server

#
# Post Processing Options
#
# Supported post processing:
#
# 1) report_title in feed hits
#
# Post processing requires cb_server_url, api_verify_ssl, and api_token to be set.  The post processed messages are
# dispatched to retrieve additional information from the Cb Response REST API.  Once the information is retrieved they
# are placed in the ouput queue.
#
# SSL/TLS verification for connections to the Cb Response REST API
#
# api_verify_ssl=false
#
# The API Token to use when querying the Cb Response REST API
#
# api_token=

#
#remove_from_output=highlights_by_doc,stuff
#
#
#comma delimited keys will be removed from LEEF/JSON output
#
remove_from_output=


#########
# Output Options
#########

#
# Configure the specific output.
# Valid options are: 'udp', 'tcp', 'file', 'stdout', 's3' ,'http','splunk',and 'kafka'
#
#  udp - Have the events sent over a UDP socket
#  tcp - Have the events sent over a TCP socket
#  file - Output the events to a rotating file
#  s3 - Place in S3 bucket (not officially supported)
#  syslog - Send the events to a syslog server
#
output_type=kafka

# Configure the output format
# valid options are: 'leef', 'json'
#
# default is 'json'
# Use 'leef' for pushing events to IBM QRadar, 'json' otherwise
#
output_format=json

#
# Output specific configuration
# These only have meaning if the option
# is enabled
#

# default to /var/cb/data/event_bridge_output.json
outfile=/var/cb/data/event_bridge_output.json

# tcpout=IP:port - ie 1.2.3.5:8080
tcpout=

# udpout=IP:port - ie 1.2.3.5:8080
udpout=

# options for S3 support
# s3out: can be an S3 bucket name (defaults to us-east-1 region)
#        or (temp-file-directory):(region):(bucket-name)
#        by default, the temp-file-directory is /var/cb/data/event-forwarder.
#
# for more s3 options, see the [s3] section below.
s3out=

# options for syslog output
# syslogout:
#   uses the format <protocol>:<hostname>:<port>
#   where <protocol> can be:
#      tcp+tls:      TCP over TLS/SSL
#      tcp:          plaintext TCP
#      udp:          plaintext UDP

# example:
#   tcp+tls:syslog.company.com:514
syslogout=

# options for HTTP output
# httpout:
#   uses the format <temporary file location>:<HTTP URL>
#   where the temporary file location is optional; defaults to /var/cb/data/event-forwarder
#
# for more http options, see the [http] section below.
#
# examples:
#   httpout=/tmp/http_out:https://http-endpoint.company.local/api/submit
#   httpout=https://http-endpoint.company.local/api/submit
httpout=

# options for splunk output
# splunkout:
#   uses the format <temporary file location>:<splunk URL HEC interface URL:port>
#   where the temporary file location is optional; defaults to /var/cb/data/event-forwarder
#
# for more http options, see the [splunk] section below.
#
# examples:
#   splunkpout=https://<splunk-server hostname or ip>:8088/services/collector/event
splunkout=
#########
# Configuration for which events are captured
#
# To specify multiple events use comma as a separator
# e.g. events_raw_sensor=ingress.event.process,ingress.event.procstart
#
#
# For more info on supported event types see
# https://github.com/carbonblack/cbapi/tree/master/server_apis
#
# Note: To enable raw sensor events you must also edit the Cb Response config
#  file (cb.conf) and enable the appropriate events by changing the config
#  option DatastoreBroadcastEventTypes.
#
# If you intend to enable raw sensor events on a Cb Enterprise Response 5.2+ server,
#  consider using the "raw sensor exchange" for enhanced performance and reduced
#  load on the Cb server. The raw exchange forwards the compressed data received
#  from the endpoint sensor unchanged to the event-forwarder. The raw sensor
#  exchange should be used if you are forwarding moduleload, filemod, or regmod
#  endpoint events as these represent the vast majority of the event types.
#
# To use the "raw sensor exchange", first uncomment the "use_raw_sensor_exchange"
#  option below:
#use_raw_sensor_exchange=true
#
#  then enable the "raw sensor exchange" in cb.conf:
#  EnableRawSensorDataBroadcast=true
#
# Note: To enable binaryinfo. events you need to enable EnableSolrBinaryInfoNotifications=True
#  within the cb.conf
#########

# Raw Sensor (endpoint) Events
# Includes:
#   ingress.event.process
#   ingress.event.procstart
#   ingress.event.netconn
#   ingress.event.procend
#   ingress.event.childproc
#   ingress.event.moduleload
#   ingress.event.module
#   ingress.event.filemod
#   ingress.event.regmod
#   ingress.event.tamper
#   ingress.event.crossprocopen
#   ingress.event.remotethread
#   ingress.event.processblock
#   ingress.event.emetmitigation
#   ALL for all of the above
#   0 - to disable all raw sensor events.
events_raw_sensor=ALL

# Watchlist Hits
# Includes:
#  watchlist.hit.process
#  watchlist.hit.binary
#  watchlist.storage.hit.process
#  watchlist.storage.hit.binary
# Note: As of version 5.2, the routing keys are different in RabbitMQ
# if you want to only subscribe to watchlist.storage.hit.process (for example),
# your configuration should be
# events_watchlist=watchlist.*.storage.hit.process note the '*' after the '.'
# Internally all watchlists show up with their database ID
# ex: watchlist.12.storage.hit.process, you'll miss them without the '*'
# (asterisk)
events_watchlist=ALL

# Feed Hits
# Includes:
#   feed.ingress.hit.process
#   feed.ingress.hit.binary
#   feed.ingress.hit.host
#   feed.storage.hit.process
#   feed.storage.hit.binary
#   feed.query.hit.process
#   feed.query.hit.binary
#   ALL for all of the above
#   0 - to disable all raw sensor events
# Note: As of version 5.2, the routing keys are different in RabbitMQ
# if you want to only subscribe to feed.storage.hit.process (for example), your
# configuration should be
# events_feed=feed.*.storage.hit.process note the '*' after the '.'
# Internally all feeds show up with their database ID
# ex: feed.12.storage.hit.process, you'll miss them without the '*' (asterisk)
events_feed=ALL

# Alert Events
# Includes:
#   alert.watchlist.hit.ingress.process
#   alert.watchlist.hit.ingress.binary
#   alert.watchlist.hit.ingress.host
#   alert.watchlist.hit.query.process
#   alert.watchlist.hit.query.binary
#   ALL for all of the above
#   0 - to disable all alert events
events_alert=ALL

# Binary Observed Events
# Includes:
#   binaryinfo.observed
#   binaryinfo.host.observed
#   binaryinfo.group.observed
events_binary_observed=ALL

# Binary Upload Events
# Includes:
#   binarystore.file.added
events_binary_upload=ALL

# Time partitioning events
# Includes:
#   events.partition.creating
#   events.partition.created
#   events.partition.mounted
#   events.partition.unmounted
#   events.partition.purged
#   events.partition.deleted
#   ALL for all of the above
#   0 - to disable all storage partitioning events
events_storage_partition=ALL

#########
# S3 configuration section
#
# The following are advanced configuration options for uploading output to Amazon S3 buckets.
#########

[s3]
# By default the S3 output type will initiate a connection to the remote service every five minutes, or when
#  the temporary file containing the event output reaches 10MB.

# Set the default timeout period in seconds. By default, cb-event-forwarder will contact the remote service every five
#  minutes (300 seconds)
# bundle_send_timeout=300

# Send empty updates? By default, cb-event-forwarder will send an empty update every bundle_send_timeout seconds.
#  if this is set to false, then the cb-event-forwarder will not initiate a connection to the remote service unless
#  there are events to send.
# upload_empty_files=true

# Set the maximum file size before the events must be flushed to the remote service. The default is 10MB.
# bundle_size_max=10485760

# Uncomment server_side_encryption below to enable SSE on uploaded files to your S3 bucket
# server_side_encryption=AES256

# Set the following ACL policy on all files uploaded to your S3 bucket
# acl_policy=bucket-owner-full-control

# Use the following credential profile to connect to S3.
# See http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html for more information on
# AWS credential storage and credential profiles. By default, the S3 output will use the default
# credential storage location of $HOME/.aws/credentials with the "default" credential profile.
#
# The credential_profile can be specified as the profile name, using the default storage location, or
# you can also specify the filename where the credentials are stored by using a colon separated format:
# for example, to look for the AWS credentials in the file /etc/cb/aws.creds, and use profile "production":
# set credential_profile to /etc/cb/aws.creds:production.

# credential_profile=default

# Use the following to create event forwarder logs under a specified object prefix
# If specified logs will be stored under <bucketname>/<object_prefix>/event-forwarder.<timestamp>
# This is useful if multiple forwarders are to use the same s3 bucket
# object_prefix=objectname

# Enables "dual stack" endpoints for the S3 client. This is necessary for environments that only have
# ipv6 networking. Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/dual-stack-endpoints.html
# use_dual_stack=true

[syslog]
# Uncomment ca_cert to specify a file containing PEM-encoded CA certificates for verifying the peer
# server when using TLS+TCP syslog
# ca_cert=/etc/cb/integrations/event-forwarder/ca-certs.pem

# Uncomment server_cname to force a specific server Common Name when validating the peer server
# certificate. Useful for situations where the peer server is using a self-signed certificate with
# a generic name (for example, QRadar's default certificate uses "*" as the CN)
# server_cname=peer.server.name

# Uncomment tls_verify and set to "false" in order to disable verification of the peer server certificate
# when using TLS+TCP syslog
# tls_verify=false

# Uncomment insecure_tls to allow connections to servers that only support TLS versions less than 1.2.
# The default is to only support TLSv1.2 cipher suites. This option will allow connections to TLSv1.0 and 1.1
# servers as well.
# insecure_tls=true

# Uncomment client_key and client_cert and set to files containing PEM-encoded private key and public
# certificate when using client TLS certificates when using TLS+TCP syslog
# client_key=/etc/cb/integrations/event-forwarder/client-key.pem
# client_cert=/etc/cb/integrations/event-forwarder/client-cert.pem

[http]
# By default the HTTP POST output type will initiate a connection to the remote service every five minutes, or when
#  the temporary file containing the event output reaches 10MB.

# Set the default timeout period in seconds. By default, cb-event-forwarder will contact the remote service every five
#  minutes (300 seconds)
# bundle_send_timeout=300

# Send empty updates? By default, cb-event-forwarder will send an empty update every bundle_send_timeout seconds.
#  if this is set to false, then the cb-event-forwarder will not initiate a connection to the remote service unless
#  there are events to send.
# upload_empty_files=true

# Set the maximum file size before the events must be flushed to the remote service. The default is 10MB.
# bundle_size_max=10485760

# Override the default template used for posting JSON to the remote service.
# The template language is Go's text/template; see https://golang.org/pkg/text/template/
# The following placeholders can be used:
#  {{.FileName}} - the filename of the current event-forwarder file being uploaded (for example event-forwarder.2016-08-11T01:01:01Z)
#  {{.Events}} - the list of events being uploaded as a "range". Each event has the following placeholder:
#   {{.EventText}} - the event itself, as a JSON dictionary.
#                    Note that a comma is added to each event (except the last) to make the list proper JSON.
# The default template for JSON is:
# http_post_template={"filename": "{{.FileName}}", "service": "carbonblack", "alerts":[{{range .Events}}{{.EventText}}{{end}}]}

# Override the content-type sent to the remote service through the HTTP Content-Type header.
#  The default content-type for JSON output is application/json.
# content_type=application/json

# Uncomment ca_cert to specify a file containing PEM-encoded CA certificates for verifying the peer server
# ca_cert=/etc/cb/integrations/event-forwarder/ca-certs.pem

# Uncomment server_cname to force a specific server Common Name when validating the peer server
# certificate. Useful for situations where the peer server is using a self-signed certificate with
# a generic name (for example, QRadar's default certificate uses "*" as the CN)
# server_cname=peer.server.name

# Uncomment tls_verify and set to "false" in order to disable verification of the peer server certificate
# tls_verify=false

# Uncomment insecure_tls to allow connections to servers that only support TLS versions less than 1.2.
# The default is to only support TLSv1.2 cipher suites. This option will allow connections to TLSv1.0 and 1.1
# servers as well.
# insecure_tls=true

# Uncomment client_key and client_cert and set to files containing PEM-encoded private key and public
#  certificate when using client TLS certificates
# client_key=/etc/cb/integrations/event-forwarder/client-key.pem
# client_cert=/etc/cb/integrations/event-forwarder/client-cert.pem

# Uncomment authorization_token to place a value in the outgoing HTTP "Authorization" header
#  (used in HTTP Basic Authentication). See https://en.wikipedia.org/wiki/Basic_access_authentication
#  for more information. By default no Authorization header is sent.
# authorization_token=Basic QWxhZGRpbjpPcGVuU2VzYW1l

# Uncomment fields which prefixed with oauth_jwt_ to use the OAuth 2.0 JSON Web Token flow, commonly
# known as "two-legged OAuth 2.0". See: https://tools.ietf.org/html/draft-ietf-oauth-jwt-bearer-12
# for more information.
#
# [Required OAuth field] oauth_jwt_client_email is the Oauth client identifier used when
# communicating with the configured OAuth provider.
# oauth_jwt_client_email=example@serviceaccount.com

# [Required OAuth field] oauth_jwt_private_key contains the contents of a RSA private key or the
# contents of a PEM file that contains a private key. The provided private key is used to sign JWT
# payloads. PEM containers with a passphrase are not supported. Use the following command to convert
# a PKCS 12 file into a PEM.
# $ openssl pkcs12 -in key.p12 -out key.pem -nodes
# oauth_jwt_private_key=private_key

# [Required OAuth field] oauth_jwt_token_url is the endpoint required to complete the 2-legged JWT
# flow.
# oauth_jwt_token_url=https://example.com/oauth2/token

# [Optional OAuth field] oauth_jwt_private_key_id contains a hint indicating which key is being
# used.
# oauth_jwt_private_key_id=private_key_id

# [Optional OAuth field] oauth_jwt_scopes specifies a comma-separated list of requested permission
# scopes.
# oauth_jwt_scopes=scope_1,scope_2

# Uncomment event_text_as_json_byte_array to output EventText as byte array field (i.e. binary field)
# when posting JSON to the remote service.
# Definition of EventText field in UploadEvent struct can be found in http_util.go.
# By default, EventText is output as JSON string field and the value of EventText will be in UTF-8
# encoded string. If EventText is output as JSON byte array field, the value of EventText will be
# in base64-encoded string.
# event_text_as_json_byte_array=false

# Change compress_http_payload to 'true' if you want to compress the HTTP POST payload to
# the remote server. This will *greatly* decrease bandwidth requirements when uploading
# large volumes of events to remote endpoints.
#
# The compression method we use is 'gzip' so we set 'Content-Encoding: gzip' in the header.
# Before turning this option on, ensure that your HTTP server can accept gzip data.
#
# The default is 'false':
compress_http_payload=false

[kafka]
# Uncomment when using kafka brokers. Broker configuration should be a comma-separated
# list of brokers with ports.
# e.g.
# brokers=kakfa01:9092,kafka02:9092
# It is not necessary to put all of your brokers here, just enough to dependably
# bootstrap the producer
brokers=kafka-1:19092,kafka-2:29092,kafka-3:29092
# Optional additional suffix to add to the name of the topics.
# By default the topic published to will be the type of the event, e.g. "procstart'
# Must be uncommented when using kafka output but can be left blank
topic_suffix=-test

# Optional custom kafka topic
# topic = mytopic

# Optional kafka "security.protocol", "sasl.mechanism", username and password
# You need to set all of them if "protocol" is set
# protocol = SASL_SSL
# mechanism = SCRAM-SHA-512
# username = kafkauser
# password = kafkapass

#Optional config option controlling sarama.MaxRequestSize
#
#max_request_size=10000000
[splunk]
# Uncomment ca_cert to specify a file containing PEM-encoded CA certificates for verifying the peer server
# ca_cert=/etc/cb/integrations/event-forwarder/ca-certs.pem

# Uncomment server_cname to force a specific server Common Name when validating the peer server
# certificate. Useful for situations where the peer server is using a self-signed certificate with
# a generic name (for example, QRadar's default certificate uses "*" as the CN)
# server_cname=peer.server.name

# Uncomment tls_verify and set to "false" in order to disable verification of the peer server certificate
# tls_verify=false

# Uncomment insecure_tls to allow connections to servers that only support TLS versions less than 1.2.
# The default is to only support TLSv1.2 cipher suites. This option will allow connections to TLSv1.0 and 1.1
# servers as well.
# insecure_tls=true

# Uncomment client_key and client_cert and set to files containing PEM-encoded private key and public
#  certificate when using client TLS certificates
# client_key=/etc/cb/integrations/event-forwarder/client-key.pem
# client_cert=/etc/cb/integrations/event-forwarder/client-cert.pem

# Set the default timeout period in seconds. By default, cb-event-forwarder will contact the remote service every five
#  minutes (300 seconds)
# bundle_send_timeout=300

# Send empty updates? By default, cb-event-forwarder will send an empty update every bundle_send_timeout seconds.
#  if this is set to false, then the cb-event-forwarder will not initiate a connection to the remote service unless
#  there are events to send.
# upload_empty_files=true

# Set the maximum file size before the events must be flushed to the remote service. The default is 10MB.
# bundle_size_max=10485760
#HEC TOKEN
#
#hec_token stores the HEC token to be used when communicating with splunk
#
hec_token=PASSWORD
